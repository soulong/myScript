#!/usr/bin/env bash
# CUT&Tag Analysis Pipeline
# Author: Hao He
# Usage: run_cuttag.sh config.yml

set -euo pipefail

# Fail fast if no config provided
if [[ ${#@} -lt 1 ]]; then
    echo "Usage: $0 config.yml" >&2
    exit 1
fi

# ============================= Helper Functions =============================
# Get script directory (works with symlinks)
SCRIPT_DIR="$(cd "$(dirname "$(readlink -f "${BASH_SOURCE[0]}")")" && pwd)"

# Helper functions are located in the parent directory
HELPER_SCRIPT="${SCRIPT_DIR}/../helper.sh"
if [[ ! -f "$HELPER_SCRIPT" ]]; then
    echo "ERROR: Helper script not found: $HELPER_SCRIPT" >&2
    exit 1
fi
source "$HELPER_SCRIPT"

# ============================= Configuration Loading =============================
parse_yaml_config() {
    local config="$1"
    
    # Simple YAML parser (handles key: value pairs)
    while IFS=': ' read -r key value || [[ -n "$key" ]]; do
        # Skip comments and empty lines
        [[ "$key" =~ ^[[:space:]]*# ]] && continue
        [[ -z "$key" ]] && continue
        
        # Remove leading/trailing whitespace and quotes
        key=$(echo "$key" | xargs)
        value=$(echo "$value" | xargs | sed 's/^["'\'']\|["'\'']$//g')
        
        # Skip empty values
        [[ -z "$value" ]] && continue
        
        # Map config keys to variables
        case "$key" in
            species) SPECIES="$value" ;;
            fastq_dir) FASTQ_DIR="$value" ;;
            samplesheet) SAMPLESHEET="$value" ;;
            outdir) OUTDIR="$value" ;;
            threads) THREADS="$value" ;;
            skip_preprocess) SKIP_PREPROCESS="$value" ;;
            sort_mem_limit) SORT_MEM_LIMIT="$value" ;;
            max_frag_length) MAX_FRAG_LENGTH="$value" ;;
            call_peak) CALL_PEAK="$value" ;;
            macs_parameter) MACS_PARAMETER="$value" ;;
            run_spike) RUN_SPIKE="$value" ;;
            norm_method) NORM_METHOD="$value" ;;
            index_rootdir) INDEX_ROOTDIR="$value" ;;
            spike_free_sf_file) SPIKE_FREE_SF_FILE="$value" ;;
            bed_region) 
                # Add custom BED regions to array
                if [[ -f "$value" ]]; then
                    CUSTOM_BED_REGIONS+=("$value")
                    log_info "Added custom BED region: $value"
                else
                    log_warn "Custom BED file not found: $value, skipped"
                fi
                ;;
        esac
    done < "$config"

    # Peak file extension used by MACS3: broad -> broadPeak, otherwise narrowPeak
    if [[ "$MACS_PARAMETER" =~ --broad ]]; then
        PEAK_EXT="_peaks.broadPeak"
    else
        PEAK_EXT="_peaks.narrowPeak"
    fi
} 


# ============================= Sample Processing Functions =============================
bowtie2_alignment() {
    local sample="$1"
    local clean1="$FASTP_DIR/${sample}_R1.fq.gz"
    local clean2="$FASTP_DIR/${sample}_R2.fq.gz"
    
    local sorted_bam="$ALIGN_DIR/${sample}.sorted.bam"
    local markdup_bam="$ALIGN_DIR/${sample}.markdup.bam"
    local flagstat_file="$ALIGN_DIR/${sample}.markdup.flagstat"
    
    if [[ ! -s "$markdup_bam" || ! -s "${markdup_bam}.bai" ]]; then
        log_info "[$sample] Aligning to genome"
        bowtie2 -x "$BOWTIE2_INDEX" -1 "$clean1" -2 "$clean2" -p "$THREADS" --phred33 \
            --very-sensitive --local --no-mixed --no-discordant -I 10 -X "$MAX_FRAG_LENGTH" \
            --rg-id "$sample" --rg "SM:${sample}\tPL:ILLUMINA" \
            2>"$ALIGN_DIR/${sample}.bowtie2.log" \
            | sambamba view -t "$THREADS" -S -f bam /dev/stdin 2>/dev/null \
            | sambamba sort -t "$THREADS" -m "$SORT_MEM_LIMIT" -o "$sorted_bam" /dev/stdin 2>/dev/null
        
        log_info "[$sample] Marking duplicates"
        sambamba markdup -t "$THREADS" "$sorted_bam" "$markdup_bam" 2>/dev/null
        sambamba index -t "$THREADS" "$markdup_bam" 2>/dev/null
        sambamba flagstat "$markdup_bam" > "$flagstat_file" 2>/dev/null
        [[ "$(wc -c < ${flagstat_file})" -gt 5 ]] && rm "$sorted_bam" "${sorted_bam}.bai"
    fi
    
    # Extract statistics
    local total_reads=$(extract_flagstat_value "$flagstat_file" "in total" 1)
    local mapped_reads=$(extract_flagstat_value "$flagstat_file" "mapped (.*N/A)" 1)
    local duplicate_reads=$(extract_flagstat_value "$flagstat_file" "duplicates" 1)
    
    echo "$total_reads $mapped_reads $duplicate_reads"
}

bowtie2_alignment_spike() {
    local sample="$1"
    local clean1="$FASTP_DIR/${sample}_R1.fq.gz"
    local clean2="$FASTP_DIR/${sample}_R2.fq.gz"
    
    local spike_sorted_bam="$ALIGN_DIR/${sample}.spike.sorted.bam"
    local spike_flagstat="$ALIGN_DIR/${sample}.spike.sorted.flagstat"
    
    if [[ ! -s "$spike_flagstat" ]]; then
        log_info "[$sample] Aligning to spike-in genome"
        bowtie2 -x "$SPIKE_INDEX" -1 "$clean1" -2 "$clean2" -p "$THREADS" --phred33 \
            --very-sensitive --local --no-mixed --no-discordant -I 10 -X "$MAX_FRAG_LENGTH" \
            --rg-id "$sample" --rg "SM:${sample}\tPL:ILLUMINA" \
            2>"$ALIGN_DIR/${sample}.spike.bowtie2.log" \
            | sambamba view -t "$THREADS" -S -f bam /dev/stdin 2>/dev/null \
            | sambamba sort -t "$THREADS" -m "$SORT_MEM_LIMIT" \
                -o "$spike_sorted_bam" /dev/stdin 2>/dev/null
        
        sambamba flagstat "$spike_sorted_bam" > "$spike_flagstat" 2>/dev/null
        [[ "$(wc -c < ${spike_flagstat})" -gt 5 ]] && rm "$spike_sorted_bam" "${spike_sorted_bam}.bai"
    fi
    
    local spike_reads=$(extract_flagstat_value "$spike_flagstat" "mapped (.*N/A)" 1)
    echo "$spike_reads"
}

bam_filter() {
    local sample="$1"
    local markdup_bam="$ALIGN_DIR/${sample}.markdup.bam"
    local filtered_bam="$ALIGN_DIR/${sample}.filtered.bam"
    local flagstat_filtered="$ALIGN_DIR/${sample}.filtered.flagstat"
    
    if [[ ! -s "$flagstat_filtered" ]]; then
        # unmapped (4), mate unmapped (8), secondary (256), failed QC (512), duplicate (1024) → 4+8+256+512+1024 = 1804
        log_info "[$sample] Filtering BAM (MAPQ≥30, proper pairs, no chrM/dups)"
        samtools view -@ "$THREADS" -b -f 2 -q 30 -F 1804 \
            -e 'rname != "chrM" && ! (rname =~ "^(GL|KI|JH|MU|chrUn|random|alt)")' \
            -o "$filtered_bam" "$markdup_bam"
        sambamba index -t "$THREADS" "$filtered_bam" 2>/dev/null
        sambamba flagstat -t "$THREADS" "$filtered_bam" > "$flagstat_filtered" 2>/dev/null
    fi
    
    local filtered_reads=$(extract_flagstat_value "$flagstat_filtered" "mapped (.*N/A)" 1)
    echo "$filtered_reads"
}

generate_bigwig() {
    local sample="$1" scale_factor="$2"
    local filtered_bam="$ALIGN_DIR/${sample}.filtered.bam"
    local bigwig="$ALIGN_DIR/${sample}.${NORM_METHOD}.bw"
    
    if [[ ! -s "$bigwig" ]]; then
        log_info "[$sample] Generating BigWig (scale: $scale_factor)"
        bamCoverage -b "$filtered_bam" -o "$bigwig" \
            -p "$THREADS" --binSize 10 --scaleFactor "$scale_factor" 2>/dev/null
    fi
}


run_alignment() {
    local sample="$1" group="$2" control="$3" target="$4" fq1="$5" fq2="$6"
    
    # Skip if already processed
    if grep -q "^${sample}," "$STATS_CSV"; then
        log_info "[$sample] Alignment completed"
        return 0
    fi
    
    # Alignment and marking duplicates
    read total_reads mapped_reads duplicate_reads <<< $(bowtie2_alignment "$sample")
    local mapped_pct dup_pct
    mapped_pct=$(awk -v m="$mapped_reads" -v t="$total_reads" 'BEGIN{if(t==0){printf "0.00"} else {printf "%.2f", (m/t)*100}}')
    dup_pct=$(awk -v d="$duplicate_reads" -v t="$total_reads" 'BEGIN{if(t==0){printf "0.00"} else {printf "%.2f", (d/t)*100}}')
    log_info "[$sample] Stats - Total: $total_reads, Mapped: $mapped_reads (${mapped_pct}%), Duplicates: $duplicate_reads (${dup_pct}%)"
    
    # Filtering
    local filtered_reads=$(bam_filter "$sample")
    filtered_pct=$(awk -v d="$filtered_reads" -v t="$total_reads" 'BEGIN{if(t==0){printf "0.00"} else {printf "%.f", (d/t)*100}}')
    log_info "[$sample] Filtered reads: $filtered_reads (${filtered_pct}%)"

    # Spike-in alignment (optional)
    local spike_reads=0
    if [[ "$RUN_SPIKE" == true ]]; then
        spike_reads=$(bowtie2_alignment_spike "$sample")
        spike_pct=$(awk -v d="$spike_reads" -v t="$total_reads" 'BEGIN{if(t==0){printf "0.000"} else {printf "%.3f", (d/t)*100}}')
        log_info "[$sample] Spike-in reads: $spike_reads (${spike_pct}%)"
    fi
    
    # Calculate normalization scale factor
    local spike_free_file="$ALIGN_DIR/SpikeFree_SF.txt"
    local scale_factor
    scale_factor=$(calculate_scale_factor "$NORM_METHOD" "$filtered_reads" "$spike_reads" "$sample" "$spike_free_file")
    log_info "[$sample] Normalization: $NORM_METHOD, Scale factor: $scale_factor"
    
    # Generate BigWig
    generate_bigwig "$sample" "$scale_factor"
    
    # Append to statistics (frip will be updated later)
    printf '%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,\n' \
        "$sample" "$group" "$control" "$target" "$fq1" "$fq2" \
        "$total_reads" "$mapped_reads" "$duplicate_reads" "$filtered_reads" \
        "$spike_reads" "$NORM_METHOD" "$scale_factor" >> "$STATS_CSV"
}

# ============================= Peak Calling =============================
run_macs() {
    local sample="$1" group="$2" control="$3" target="$4"
    
    local filtered_bam="$ALIGN_DIR/${sample}.filtered.bam"
    local peak_file="$PEAK_DIR/${sample}${PEAK_EXT}"
    
    if [[ ! -s "$peak_file" ]]; then
        log_info "[$sample] Running macs"
        
        # Build MACS3 arguments
        local -a macs_args=("-t" "$filtered_bam" "-f" "BAMPE" "-g" "$GSIZE" "-n" "$PEAK_DIR/${sample}" "--keep-dup" "all")
        # Split MACS parameters string into array and append
        read -ra macs_params_arr <<< "$MACS_PARAMETER"
        macs_args+=("${macs_params_arr[@]}")
        
        if [[ -n "$control" ]]; then
            local control_bam="$ALIGN_DIR/${control}.filtered.bam"
            if [[ -f "$control_bam" ]]; then
                macs_args+=("-c" "$control_bam")
            else
                log_warn "[$sample] Control BAM not found: $control_bam"
            fi
        fi
        
        macs3 callpeak "${macs_args[@]}" 2>/dev/null
    else
        log_info "[$sample] Call peak completed"
    fi
} 

# ============================= Peak Consensus =============================
run_consensus() {
    # Collect samples by target
    declare -A target_peaks
    declare -a all_peak_files

    while IFS=, read -r sample group control target fq1 fq2 || [[ -n "${sample:-}" ]]; do
        [[ "$sample" == "sample" ]] && continue
        [[ -z "$sample" ]] && continue

        local peak_file="$PEAK_DIR/${sample}${PEAK_EXT}"
        [[ ! -f "$peak_file" ]] && continue

        all_peak_files+=("$peak_file")

        if [[ -n "$target" ]]; then
            if [[ -z "${target_peaks[$target]:-}" ]]; then
                target_peaks[$target]="$peak_file"
            else
                target_peaks[$target]="${target_peaks[$target]} $peak_file"
            fi
        fi
    done < <(tail -n +2 "$SAMPLESHEET" | tr -d '\r')

    # Create consensus for each target
    for target in "${!target_peaks[@]}"; do
        local -a peaks=()
        read -ra peaks <<< "${target_peaks[$target]}"
        # echo "$target: ${peaks[@]}"

        if [[ ${#peaks[@]} -eq 0 ]]; then
            log_warn "No peak files for target: $target"
            continue
        fi

        local consensus_bed="${PEAK_DIR}/consensus_${target}.bed"
        if [[ ! -s "$consensus_bed" ]]; then
            log_info "[$target] Make consensus from ${#peaks[@]} samples"
            # log_info "Samples: ${peaks[@]}"
            merge_peakfiles --input-peaks "${peaks[*]}" --output-bed "$consensus_bed" --chrom-size "$CHROM_SIZES" || \
                { 
                    log_warn "Failed to create consensus for: $target"
                    continue
                }
        else 
            log_info "[$target] Consensus peak completed"
        fi
    done

    # Create consensus for all samples, only if N target > 1
    if [[ ${#target_peaks[@]} -gt 1 ]] && [[ ! -s "${PEAK_DIR}/consensus_all.bed" ]]; then
        local consensus_bed="${PEAK_DIR}/consensus_all.bed"
        log_info "Creating consensus peaks for: all (${#all_peak_files[@]} samples)"
        merge_peakfiles --input-peaks "${all_peak_files[*]}" --output-bed "$consensus_bed" --chrom-size "$CHROM_SIZES" || \
            log_warn "Failed to create consensus for: all"
    fi
}



calculate_frip() {
    local prefix="$1"
    local consensus_bed="$2"
    shift 2

    local -a bam_array=("$@")
    if [[ ${#bam_array[@]} -eq 0 || ! -f "$consensus_bed" ]]; then
        return 0
    fi
    local -a name_array=()
    for bam in "${bam_array[@]}"; do
        name_array+=("$(basename "$bam" .filtered.bam)")
    done
    
    local frip_tab="$MULTIQC_DIR/consensus_${prefix}_frip.tab"
    local frip_plot="$MULTIQC_DIR/consensus_${prefix}_frip.pdf"
    if [[ ! -s "$frip_tab" ]]; then
        log_info "[QC] Computing FRiP: $prefix"
        plotEnrichment -p "$THREADS" -b "${bam_array[@]}" \
            --BED "$consensus_bed" --labels "${name_array[@]}" --outRawCounts "$frip_tab" --extendReads \
            --plotFile "$frip_plot" --plotTitle "FRiP - $prefix (Consensus Peaks)" >/dev/null
    fi
}

profile_heatmap() {
    local prefix="$1"
    local bed_file="$2"
    local type="$3"
    shift 3

    local -a bw_array=("$@")
    if [[ ${#bw_array[@]} -eq 0 || ! -f "$bed_file" ]]; then
        return 0
    fi
    local -a name_array=()
    for bw in "${bw_array[@]}"; do
        name_array+=("$(basename "$bw" .${NORM_METHOD}.bw)")
    done
        
    local bed_name=$(basename "$bed_file" .bed)
    log_info "[Heatmap QC] $prefix ($type) over $bed_name"
    
    # TSS heatmap
    if [[ "$type" == "tss" || "$type" == "both" ]]; then
        local heatmap_tss="$MULTIQC_DIR/${bed_name}_${prefix}_tss_${NORM_METHOD}_heatmap.pdf"
        if [[ ! -s "$heatmap_tss" ]]; then
            local mat_tss="$MULTIQC_DIR/${bed_name}_${prefix}_tss_${NORM_METHOD}_mat.gz"
            [[ ! -s "$mat_tss" ]] && \
                computeMatrix reference-point --referencePoint TSS -S "${bw_array[@]}" -R "$bed_file" \
                    -o "$mat_tss" -p "$THREADS" -b 3000 -a 3000 --skipZeros \
                    --samplesLabel "${name_array[@]}" --quiet >/dev/null
            plotHeatmap -m "$mat_tss" -out "$heatmap_tss" --colorMap Blues >/dev/null
        fi
    fi

    # Center heatmap
    if [[ "$type" == "center" || "$type" == "both" ]]; then
        local heatmap_center="$MULTIQC_DIR/${bed_name}_${prefix}_center_${NORM_METHOD}_heatmap.pdf"
        if [[ ! -s "$heatmap_center" ]]; then
            local mat_center="$MULTIQC_DIR/${bed_name}_${prefix}_center_${NORM_METHOD}_mat.gz"
            [[ ! -s "$mat_center" ]] && \
                computeMatrix reference-point --referencePoint center -S "${bw_array[@]}" -R "$bed_file" \
                    -o "$mat_center" -p "$THREADS" -b 3000 -a 3000 --skipZeros \
                    --samplesLabel "${name_array[@]}" --quiet >/dev/null
            plotHeatmap -m "$mat_center" -out "$heatmap_center" --colorMap Blues >/dev/null
        fi
    fi
    
    # # Gene body heatmap (unchanged: always generated)
    # local heatmap_body="$MULTIQC_DIR/${bed_name}_${prefix}_body_${NORM_METHOD}_heatmap.pdf"
    # if [[ ! -s "$heatmap_body" ]]; then
    #     local mat_body="$MULTIQC_DIR/${bed_name}_${prefix}_body_${NORM_METHOD}_mat.gz"
    #     [[ ! -s "$mat_body" ]] && \
    #         computeMatrix scale-regions -S "${bw_array[@]}" -R "$bed_file" \
    #             -o "$mat_body" -p "$THREADS" -b 3000 -a 3000 --skipZeros \
    #             --regionBodyLength 5000 --samplesLabel "${name_array[@]}" \
    #             --quiet 2>/dev/null
    #     plotHeatmap -m "$mat_body" -out "$heatmap_body" --colorMap Blues 2>/dev/null
    # fi
}

bam_qc() {
    local -a bam_array=("$@")
    
    if [[ ${#bam_array[@]} -eq 0 ]]; then
        return 0
    fi
    
    local -a name_array=()
    for bam in "${bam_array[@]}"; do
        name_array+=("$(basename "$bam" .filtered.bam)")
    done
    
    # Fragment length distribution
    if [[ ! -s "$MULTIQC_DIR/BAM_fragment_length.pdf" ]]; then
        log_info "[BAM QC] Calculating fragment length"
        bamPEFragmentSize -b "${bam_array[@]}" -p "$THREADS" --maxFragmentLength "$MAX_FRAG_LENGTH" \
            --histogram "$MULTIQC_DIR/BAM_fragment_length.pdf" --samplesLabel "${name_array[@]}" \
            --outRawFragmentLengths "$MULTIQC_DIR/BAM_fragment_length.txt" >/dev/null
    fi
    
    # Fingerprint
    if [[ ! -s "$MULTIQC_DIR/BAM_fingerprint.pdf" ]]; then
        log_info "[BAM QC] Generating fingerprint"
        plotFingerprint -b "${bam_array[@]}" -p "$THREADS" --labels "${name_array[@]}" --skipZeros \
            --plotFile "$MULTIQC_DIR/BAM_fingerprint.pdf" --outRawCounts "$MULTIQC_DIR/BAM_fingerprint.txt" >/dev/null 2>&1
    fi
}

bigwig_qc() {
    local prefix="$1"
    shift
    local -a bw_array=("$@")
    
    if [[ ${#bw_array[@]} -eq 0 ]]; then
        return 0
    fi
    
    local -a name_array=()
    for bw in "${bw_array[@]}"; do
        name_array+=("$(basename "$bw" .${NORM_METHOD}.bw)")
    done
    
    local npz_file="$MULTIQC_DIR/BigWig_${prefix}_${NORM_METHOD}.npz"
     
    # Compute summary
    if [[ ! -s "$npz_file" ]]; then
        log_info "[BigWig QC] Computing BigWig"
        multiBigwigSummary bins -b "${bw_array[@]}" -p "$THREADS" \
            --labels "${name_array[@]}" -o "$npz_file" 2>/dev/null
    fi
    
    # PCA plot
    if [[ ! -s "$MULTIQC_DIR/BigWig_${prefix}_${NORM_METHOD}_PCA.pdf" ]]; then
        log_info "[BigWig QC] Generating PCA"
        plotPCA -in "$npz_file" -o "$MULTIQC_DIR/BigWig_${prefix}_${NORM_METHOD}_PCA.pdf" >/dev/null
    fi
    
    # Correlation heatmap
    if [[ ! -s "$MULTIQC_DIR/BigWig_${prefix}_${NORM_METHOD}_correlation.pdf" ]]; then
        log_info "[BigWig QC] Generating Correlation"
        plotCorrelation -in "$npz_file" --corMethod spearman --whatToPlot heatmap \
            -o "$MULTIQC_DIR/BigWig_${prefix}_${NORM_METHOD}_correlation.pdf" >/dev/null
    fi
}


run_global_qc() {
    # Collect all samples organized by target
    # Collect samples organized by target
    # Populates: TARGET_SAMPLES (associative array)
    declare -gA TARGET_SAMPLES
    declare -ga ALL_SAMPLES ALL_BAMS ALL_BWS ALL_TARGETS
    while IFS=, read -r sample group control target fq1 fq2 || [[ -n "${sample:-}" ]]; do
        [[ "$sample" == "sample" ]] && continue
        [[ -z "$sample" ]] && continue
        
        local bam="$ALIGN_DIR/${sample}.filtered.bam"
        local bw="$ALIGN_DIR/${sample}.${NORM_METHOD}.bw"
        
        [[ ! -f "$bam" ]] && continue
        
        ALL_SAMPLES+=("$sample")
        ALL_BAMS+=("$bam")
        ALL_BWS+=("$bw")
        
        if [[ -n "$target" ]]; then
            if [[ -z "${TARGET_SAMPLES[$target]:-}" ]]; then
                TARGET_SAMPLES[$target]="$sample"
                ALL_TARGETS+=("$target")
            else
                TARGET_SAMPLES[$target]="${TARGET_SAMPLES[$target]} $sample"
            fi
        fi
    done < <(tail -n +2 "$SAMPLESHEET" | tr -d '\r')
    
    if [[ ${#ALL_SAMPLES[@]} -eq 0 ]]; then
        log_warn "No samples found for QC"
        return 0
    fi
    
    log_info "Found ${#ALL_SAMPLES[@]} samples across ${#ALL_TARGETS[@]} targets"
    

    # BAM-level QC (all samples)
    bam_qc "${ALL_BAMS[@]}"
    
    # BigWig QC for all samples
    bigwig_qc "all_samples" "${ALL_BWS[@]}"
    
    # Heatmaps for all samples over genomic regions
    for bed_file in "${BED_REGIONS[@]}"; do
        [[ -f "$bed_file" ]] && profile_heatmap "all_samples" "$bed_file" "both" "${ALL_BWS[@]}"
    done

    # Target-specific consensus peak QC
    for target in "${ALL_TARGETS[@]}"; do
        # Get samples for this target
        local -a target_samples=() target_bams=() target_bws=()
        read -ra target_samples <<< "${TARGET_SAMPLES[$target]}"
        for sample in "${target_samples[@]}"; do
            target_bams+=("$ALIGN_DIR/${sample}.filtered.bam")
            target_bws+=("$ALIGN_DIR/${sample}.${NORM_METHOD}.bw")
        done

        # # Heatmaps for target
        # for bed_file in "${BED_REGIONS[@]}"; do
        #     [[ -f "$bed_file" ]] && profile_heatmap "$target" "$bed_file" "center" "${target_bws[@]}"
        # done
        
        # Consensus peak heatmap and FRiP for target
        local target_consensus="$PEAK_DIR/consensus_${target}.bed"
        if [[ -f "$target_consensus" ]]; then
            calculate_frip "$target" "$target_consensus" "${target_bams[@]}"
            profile_heatmap "$target" "$target_consensus" "center" "${target_bws[@]}"
        fi
    done
    
    # All-samples consensus peak QC, If only one target, skip
    if [[ ${#ALL_TARGETS[@]} -gt 1 ]]; then
        local all_consensus="$PEAK_DIR/consensus_all.bed"
        if [[ -f "$all_consensus" ]]; then
            calculate_frip "all" "$all_consensus" "${ALL_BAMS[@]}"
            # profile_heatmap "all" "$all_consensus" "center" "${ALL_BWS[@]}"
        fi
    fi
    
    # MultiQC
    log_info "[Multi QC] Generating MultiQC report"
    multiqc "$OUTDIR" --force -o "$MULTIQC_DIR" --title "CUT&Tag Analysis" 2>/dev/null || true
}


# ============================= Main Pipeline =============================
main() {
    local config_file="${1:-}"
    
    # Setup logging
    local log_file="./pipeline.log"
    exec > >(tee -a "$log_file") 2>&1
    
    log_info "============================================================"
    log_info "CUT&Tag Analysis Pipeline"
    log_info "============================================================"
    
    # Check dependencies
    require_commands fastqc fastp bowtie2 sambamba samtools macs3 deeptools bedtools multiqc

    # Load configuration
    parse_yaml_config "$config_file"

    # Setup genome
    setup_genome "$SPECIES" "$INDEX_ROOTDIR"

    # Check samplesheet
    if [[ ! -s "$SAMPLESHEET" ]]; then
        log_warn "Samplesheet not found: $SAMPLESHEET"
        log_info "Generating template samplesheet from FASTQ directory..."
        generate_samplesheet "$FASTQ_DIR" "$SAMPLESHEET"
        log_info "Please edit $SAMPLESHEET and re-run the pipeline"
        exit 0
    fi

    # Define directory paths
    FASTQC_DIR="$OUTDIR/01_fastqc"
    FASTP_DIR="$OUTDIR/02_fastp"
    ALIGN_DIR="$OUTDIR/03_bam"
    PEAK_DIR="$OUTDIR/04_peaks"
    MULTIQC_DIR="$OUTDIR/05_multiqc"
    # Create directories
    create_dirs "$OUTDIR" "$FASTQC_DIR" "$FASTP_DIR" "$ALIGN_DIR" "$PEAK_DIR" "$MULTIQC_DIR"
    
    # Initialize statistics file
    STATS_CSV="$OUTDIR/statistics_${NORM_METHOD}.csv"
    if [[ ! -f "$STATS_CSV" ]]; then
        log_info "Initializing statistics file: $STATS_CSV"
        cat > "$STATS_CSV" <<EOF
sample,group,control,target,fq1,fq2,total_reads,mapped_reads,duplicate_reads,filtered_reads,spike_reads,norm_method,scale_factor
EOF
    fi

    # Display configuration
    log_info "Configuration:"
    log_info "  Species: $SPECIES"
    log_info "  Samplesheet: $SAMPLESHEET"
    log_info "  Threads: $THREADS"
    log_info "  Normalization: $NORM_METHOD"
    log_info "  Peak calling: $CALL_PEAK ($MACS_PARAMETER)"
    log_info "  Spike-in: $RUN_SPIKE"
    log_info "============================================================"
    
    # Phase 1: Pre-processing
    if [[ "$SKIP_PREPROCESS" == true ]]; then
        log_info ""
        log_info "========== Skip Pre-processing =========="
    else
        log_info ""
        log_info "========== Phase 1: Pre-processing  =========="
        loop_over_csv "$SAMPLESHEET" run_preprocess
    fi

    # Phase 2: Alignment
    log_info ""
    log_info "========== Phase 2: Alignment =========="
    loop_over_csv "$SAMPLESHEET" run_alignment
    
    # Phase 3: Peak Calling
    if [[ "$CALL_PEAK" == true ]]; then
        log_info ""
        log_info "========== Phase 3: Peak Calling =========="
        loop_over_csv "$SAMPLESHEET" run_macs
        
        # Consensus Peaks
        log_info "========== Consensus Peaks =========="
        run_consensus
    fi
    
    # Phase 4: QC
    log_info ""
    log_info "========== Phase 4: QC =========="
    run_global_qc
    
    log_info ""
    log_info "============================================================"
    log_info "Pipeline completed successfully!"
    log_info "============================================================"
}


# ============================= Entry Point =============================
main "$@"